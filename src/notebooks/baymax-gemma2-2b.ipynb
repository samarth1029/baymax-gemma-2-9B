{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-02T13:20:49.413758Z","iopub.execute_input":"2024-11-02T13:20:49.414310Z","iopub.status.idle":"2024-11-02T13:23:03.895319Z","shell.execute_reply.started":"2024-11-02T13:20:49.414275Z","shell.execute_reply":"2024-11-02T13:23:03.894196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:23:12.102804Z","iopub.execute_input":"2024-11-02T13:23:12.103694Z","iopub.status.idle":"2024-11-02T13:23:31.946852Z","shell.execute_reply.started":"2024-11-02T13:23:12.103651Z","shell.execute_reply":"2024-11-02T13:23:31.945901Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:24:04.035941Z","iopub.execute_input":"2024-11-02T13:24:04.036337Z","iopub.status.idle":"2024-11-02T13:24:04.414746Z","shell.execute_reply.started":"2024-11-02T13:24:04.036300Z","shell.execute_reply":"2024-11-02T13:24:04.413827Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Gemma-2-9b-it on HealthCare Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:24:15.356606Z","iopub.execute_input":"2024-11-02T13:24:15.357020Z","iopub.status.idle":"2024-11-02T13:24:17.958544Z","shell.execute_reply.started":"2024-11-02T13:24:15.356981Z","shell.execute_reply":"2024-11-02T13:24:17.957685Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamarthmishra291-personal\u001b[0m (\u001b[33msamarthmishra291-personal-nit-rourkela\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241102_132416-bskjynk6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset/runs/bskjynk6' target=\"_blank\">prime-planet-4</a></strong> to <a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset' target=\"_blank\">https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset/runs/bskjynk6' target=\"_blank\">https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset/runs/bskjynk6</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"google/gemma-2-2b-it\"\ndataset_name = \"lavita/ChatDoctor-HealthCareMagic-100k\"\nnew_model = \"Gemma-2-2b-baymax\"","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:55:31.706308Z","iopub.execute_input":"2024-11-02T13:55:31.707203Z","iopub.status.idle":"2024-11-02T13:55:31.711241Z","shell.execute_reply.started":"2024-11-02T13:55:31.707161Z","shell.execute_reply":"2024-11-02T13:55:31.710340Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:24:46.659394Z","iopub.execute_input":"2024-11-02T13:24:46.659802Z","iopub.status.idle":"2024-11-02T13:24:46.746135Z","shell.execute_reply.started":"2024-11-02T13:24:46.659764Z","shell.execute_reply":"2024-11-02T13:24:46.745174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:31:38.963019Z","iopub.execute_input":"2024-11-02T13:31:38.963381Z","iopub.status.idle":"2024-11-02T13:31:46.592630Z","shell.execute_reply.started":"2024-11-02T13:31:38.963348Z","shell.execute_reply":"2024-11-02T13:31:46.591848Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1bc065eedad45d8b87395a1f8bf23a1"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Trying out inference","metadata":{}},{"cell_type":"code","source":"bnbConfig = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel1 = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    device_map = \"auto\",\n    quantization_config=bnbConfig\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:32:19.886028Z","iopub.execute_input":"2024-11-02T13:32:19.886651Z","iopub.status.idle":"2024-11-02T13:32:25.674123Z","shell.execute_reply.started":"2024-11-02T13:32:19.886612Z","shell.execute_reply":"2024-11-02T13:32:25.673223Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966d7beb9cf3488db0c2ec1d14459440"}},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\nsystem =  \"You are a skilled software architect who consistently creates system designs for various applications.\"\nuser = \"Design a system with the ASCII diagram for the customer support application.\"\n\nprompt = f\"System: {system} \\n User: {user} \\n AI: \"\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model1.generate(**inputs, max_length=500, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nMarkdown(text.split(\"AI:\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:32:50.315468Z","iopub.execute_input":"2024-11-02T13:32:50.316421Z","iopub.status.idle":"2024-11-02T13:33:14.923008Z","shell.execute_reply.started":"2024-11-02T13:32:50.316365Z","shell.execute_reply":"2024-11-02T13:33:14.922064Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":" \n\nI can help you design a customer support application system. However, I can't create an ASCII diagram.  \n\nTo help me design a system, please tell me:\n\n* **What is the purpose of the customer support application?** (e.g., handle incoming calls, email inquiries, live chat, etc.)\n* **What features are essential for the application?** (e.g., ticket management, knowledge base, reporting, etc.)\n* **What are the expected user roles?** (e.g., customer, agent, manager, etc.)\n* **What are the technical requirements?** (e.g., scalability, security, integration with other systems, etc.)\n\nOnce I have this information, I can provide you with a detailed system design, including:\n\n* **System architecture:** A high-level overview of the system's components and their relationships.\n* **Data model:** A description of the data structures and relationships used by the system.\n* **API design:** A specification of the interfaces used by the system to communicate with other systems.\n* **Security considerations:** A description of the security measures used to protect the system and its data.\n\nLet's start by defining the purpose and features of your customer support application. \n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning Gemma 2 Using LoRA","metadata":{}},{"cell_type":"markdown","source":"### Adding Adapter to the layer:<br>\nFine-tuning the full model will take a lot of time, so to accelerate the training process, we will create and attach the adapter layer, resulting in a faster and more memory-efficient process. \n\nThe adoption layer is created using the target modules and task type. Next, we set up the chat format for the model and tokenizer. Finally, we attach the base model to the adapter to create a Parameter Efficient Fine-Tuning (PEFT) model.","metadata":{}},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:33:24.946944Z","iopub.execute_input":"2024-11-02T13:33:24.947343Z","iopub.status.idle":"2024-11-02T13:33:24.954740Z","shell.execute_reply.started":"2024-11-02T13:33:24.947299Z","shell.execute_reply":"2024-11-02T13:33:24.953651Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:33:26.864710Z","iopub.execute_input":"2024-11-02T13:33:26.865429Z","iopub.status.idle":"2024-11-02T13:33:26.871776Z","shell.execute_reply.started":"2024-11-02T13:33:26.865387Z","shell.execute_reply":"2024-11-02T13:33:26.870819Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"modules","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:33:27.990505Z","iopub.execute_input":"2024-11-02T13:33:27.991157Z","iopub.status.idle":"2024-11-02T13:33:27.997882Z","shell.execute_reply.started":"2024-11-02T13:33:27.991115Z","shell.execute_reply":"2024-11-02T13:33:27.996954Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['k_proj', 'o_proj', 'gate_proj', 'q_proj', 'v_proj', 'up_proj', 'down_proj']"},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:33:29.056511Z","iopub.execute_input":"2024-11-02T13:33:29.057208Z","iopub.status.idle":"2024-11-02T13:33:29.120929Z","shell.execute_reply.started":"2024-11-02T13:33:29.057167Z","shell.execute_reply":"2024-11-02T13:33:29.119712Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LoRA config\u001b[39;00m\n\u001b[1;32m      2\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      3\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      4\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39mmodules\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43msetup_chat_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/models/utils.py:101\u001b[0m, in \u001b[0;36msetup_chat_format\u001b[0;34m(model, tokenizer, format, resize_to_multiple_of)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# check if model already had a chat template\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat template is already added to the tokenizer. If you want to overwrite it, please set it to None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# check if format available and retrieve\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FORMAT_MAPPING:\n","\u001b[0;31mValueError\u001b[0m: Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None"],"ename":"ValueError","evalue":"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None","output_type":"error"}]},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:34:38.923584Z","iopub.execute_input":"2024-11-02T13:34:38.924450Z","iopub.status.idle":"2024-11-02T13:34:39.554098Z","shell.execute_reply.started":"2024-11-02T13:34:38.924397Z","shell.execute_reply":"2024-11-02T13:34:39.553010Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Loading the dataset:<br>\nWe will now load the lavita/ChatDoctor-HealthCareMagic-100k dataset from the Hugging Face hub. The dataset consists of three columns:\n\n* instruction: Consists of system instruction. \n* input: Detailed patient query.\n* output: The doctor's response to the patient's query.","metadata":{}},{"cell_type":"markdown","source":"After loading the dataset, we will shuffle it and select 1000 samples to reduce the training time even further. In the end, we will create the chat format using the default chat template and use it to create the “text” column. ","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"input\"]},\n                {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:40:22.718986Z","iopub.execute_input":"2024-11-02T13:40:22.719761Z","iopub.status.idle":"2024-11-02T13:40:25.198095Z","shell.execute_reply.started":"2024-11-02T13:40:22.719721Z","shell.execute_reply":"2024-11-02T13:40:25.197144Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271862c6f0014f11913fed31bc3ace80"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'text'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:41:26.448311Z","iopub.execute_input":"2024-11-02T13:41:26.448720Z","iopub.status.idle":"2024-11-02T13:41:26.461169Z","shell.execute_reply.started":"2024-11-02T13:41:26.448677Z","shell.execute_reply":"2024-11-02T13:41:26.460222Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"\"<bos><start_of_turn>user\\nSoreness on right side directly below rib cage, not under the ribs and bad back and hip pain. What could be going on? I am 46 years old, weigh 240 and am 5 11. Last year I tore my left acl and meniscus. Received an allograft and meniscus shaving. Anxiety became present then but after medication it went away.<end_of_turn>\\n<start_of_turn>model\\nDear-thanks for using our service, I reviewed the question in details and will give you my medical advice. The pain below the rib cage can be gas, muscular of from the gallbladder. However, you are overweight and that is aggravating the pain that you are experiencing, putting all the weight on your knee aggravated the meniscus problem. Anxiety increases intestinal gas and that can give you more pain. I recommend you to have a healthy diet, free of irritants and start doing exercise daily. If after diet modification and exercise you don't feel better, you might need a reevaluation of your problem with your primary care doctor. Chat Doctor.<end_of_turn>\\n\""},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:41:36.510650Z","iopub.execute_input":"2024-11-02T13:41:36.511160Z","iopub.status.idle":"2024-11-02T13:41:36.527807Z","shell.execute_reply.started":"2024-11-02T13:41:36.511119Z","shell.execute_reply":"2024-11-02T13:41:36.527036Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Complaining and training the model:<br>\nWe will now set the training argument and STF parameters and then start the training process.","metadata":{}},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:41:44.408769Z","iopub.execute_input":"2024-11-02T13:41:44.409631Z","iopub.status.idle":"2024-11-02T13:41:44.445614Z","shell.execute_reply.started":"2024-11-02T13:41:44.409588Z","shell.execute_reply":"2024-11-02T13:41:44.444851Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:46:04.573479Z","iopub.execute_input":"2024-11-02T13:46:04.573838Z","iopub.status.idle":"2024-11-02T13:46:06.578064Z","shell.execute_reply.started":"2024-11-02T13:46:04.573803Z","shell.execute_reply":"2024-11-02T13:46:06.577201Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9261f085196d4ec187a31369a11ed5d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e71c1182c548a2a41f4d758f026933"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:46:27.316002Z","iopub.execute_input":"2024-11-02T13:46:27.316387Z","iopub.status.idle":"2024-11-02T13:54:27.735769Z","shell.execute_reply.started":"2024-11-02T13:46:27.316351Z","shell.execute_reply":"2024-11-02T13:54:27.735011Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 07:57, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>2.600200</td>\n      <td>2.718037</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.902100</td>\n      <td>2.677273</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.312200</td>\n      <td>2.646410</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.380300</td>\n      <td>2.621952</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.617100</td>\n      <td>2.612842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=2.6965235212114123, metrics={'train_runtime': 479.4825, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.939, 'total_flos': 2616512146168320.0, 'train_loss': 2.6965235212114123, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model evaluation","metadata":{}},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:54:41.081603Z","iopub.execute_input":"2024-11-02T13:54:41.082481Z","iopub.status.idle":"2024-11-02T13:54:42.575235Z","shell.execute_reply.started":"2024-11-02T13:54:41.082439Z","shell.execute_reply":"2024-11-02T13:54:42.574227Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▃▃▁▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▆█▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▆█▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▃▄▆█▂▃▃▄▄▂▁▄▄▅▂▂▂▄▄▂▃▃▃▂▅▄▅▃▃▂▃▃▄▄▂▂▂▃▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▆▆▅▇▅▅▆▆▆▆▅▆▅▆▇▅▄▅▆▇▆▆▇▄▆▄▆▄▇▅▄▅▁▃▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.61284</td></tr><tr><td>eval/runtime</td><td>16.8759</td></tr><tr><td>eval/samples_per_second</td><td>5.926</td></tr><tr><td>eval/steps_per_second</td><td>5.926</td></tr><tr><td>total_flos</td><td>2616512146168320.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>2.66102</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.6171</td></tr><tr><td>train_loss</td><td>2.69652</td></tr><tr><td>train_runtime</td><td>479.4825</td></tr><tr><td>train_samples_per_second</td><td>1.877</td></tr><tr><td>train_steps_per_second</td><td>0.939</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">prime-planet-4</strong> at: <a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset/runs/bskjynk6' target=\"_blank\">https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset/runs/bskjynk6</a><br/> View project at: <a href='https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset' target=\"_blank\">https://wandb.ai/samarthmishra291-personal-nit-rourkela/Fine-tune%20Gemma-2-9b-it%20on%20HealthCare%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241102_132416-bskjynk6/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:56:14.365888Z","iopub.execute_input":"2024-11-02T13:56:14.366285Z","iopub.status.idle":"2024-11-02T13:56:21.704578Z","shell.execute_reply.started":"2024-11-02T13:56:14.366246Z","shell.execute_reply":"2024-11-02T13:56:21.703645Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07148b9c52f54d70b657246cc33ffa37"}},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/samarth1029/Gemma-2-2b-baymax/commit/46756ee948c6cdb4f8770e6664d15ea03bbaaa35', commit_message='Upload model', commit_description='', oid='46756ee948c6cdb4f8770e6664d15ea03bbaaa35', pr_url=None, repo_url=RepoUrl('https://huggingface.co/samarth1029/Gemma-2-2b-baymax', endpoint='https://huggingface.co', repo_type='model', repo_id='samarth1029/Gemma-2-2b-baymax'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# import pkg_resources\n# pkg_resources.require(\"torch==2.3.0\")\nimport torch\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello, I am in the middle of a severe anxiety/panic attack. Could you help me?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T13:57:29.197184Z","iopub.execute_input":"2024-11-02T13:57:29.197565Z","iopub.status.idle":"2024-11-02T13:57:51.403543Z","shell.execute_reply.started":"2024-11-02T13:57:29.197523Z","shell.execute_reply":"2024-11-02T13:57:51.402594Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"user\nHello, I am in the middle of a severe anxiety/panic attack. Could you help me?\nmodel\nHi, I am Chat Doctor answering your query. I have gone through your query and understand your concern. I can understand your anxiety and panic attack. I would suggest you to take deep breaths and relax. You can also take some anti-anxiety Chat Doctor.  You can also take some anti-depressant Chat Doctor.  You can also take some anti-anxiety Chat Doctor.  You can also take some anti-depressant Chat Doctor.  You can also take some anti-anxiety Chat Doctor.  You can also take some anti-depressant Chat Doctor.  You can also take some anti-anxiety Chat Doctor.  You can also take some anti-depressant Chat Doctor.  You can also take some anti-anxiety Chat\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Merging the Base Model with Adopter<br>\nNow, we will merge the adapter with the base model and push the full model to the Hugging Face hub.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n\n\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload= AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:13:41.181276Z","iopub.execute_input":"2024-11-02T14:13:41.181680Z","iopub.status.idle":"2024-11-02T14:13:48.532083Z","shell.execute_reply.started":"2024-11-02T14:13:41.181642Z","shell.execute_reply":"2024-11-02T14:13:48.531277Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad4154b4e924eebbf7f514b25537e2c"}},"metadata":{}}]},{"cell_type":"markdown","source":"Set the chat format to the newly loaded base model and combine it with the adopter. In the end, we will load and merge the adopter to the base model. \n\nThe merge_and_unload() function will help us merge the adapter weights with the base model and use it as a standalone model.","metadata":{}},{"cell_type":"code","source":"#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:14:32.571102Z","iopub.execute_input":"2024-11-02T14:14:32.571793Z","iopub.status.idle":"2024-11-02T14:14:33.386817Z","shell.execute_reply.started":"2024-11-02T14:14:32.571751Z","shell.execute_reply":"2024-11-02T14:14:33.385743Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"Gemma-2-2b-baymax\")\ntokenizer.save_pretrained(\"Gemma-2-2b-baymax\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:15:07.630009Z","iopub.execute_input":"2024-11-02T14:15:07.630414Z","iopub.status.idle":"2024-11-02T14:15:23.181227Z","shell.execute_reply.started":"2024-11-02T14:15:07.630377Z","shell.execute_reply":"2024-11-02T14:15:23.180236Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"('Gemma-2-2b-baymax/tokenizer_config.json',\n 'Gemma-2-2b-baymax/special_tokens_map.json',\n 'Gemma-2-2b-baymax/tokenizer.model',\n 'Gemma-2-2b-baymax/added_tokens.json',\n 'Gemma-2-2b-baymax/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"Gemma-2-2b-baymax\", use_temp_dir=False)\ntokenizer.push_to_hub(\"Gemma-2-2b-baymax\", use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:18:01.603309Z","iopub.execute_input":"2024-11-02T14:18:01.604081Z","iopub.status.idle":"2024-11-02T14:20:46.916843Z","shell.execute_reply.started":"2024-11-02T14:18:01.604040Z","shell.execute_reply":"2024-11-02T14:20:46.915967Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9bd89251be94eb596ddf58e99b9a299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da66ada97ee740028dd46fd5f37d898b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d770d83f0ea4d86b4178ff13c5e2bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a26fcf9bc8411384d9de60e53f3d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45198ef46e6a49d59a43fb9c9a565ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0042d72f6c2a4d7dbf7a396aa52b47a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6a8bca5ad94c3bbb1f096257e99298"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c14a08e758a4c51aecce2baa73cec68"}},"metadata":{}},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/samarth1029/Gemma-2-2b-baymax/commit/51c81591e7b1e0c96db70dc06e3fc92664566e31', commit_message='Upload tokenizer', commit_description='', oid='51c81591e7b1e0c96db70dc06e3fc92664566e31', pr_url=None, repo_url=RepoUrl('https://huggingface.co/samarth1029/Gemma-2-2b-baymax', endpoint='https://huggingface.co', repo_type='model', repo_id='samarth1029/Gemma-2-2b-baymax'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello, I am in the middle of a severe anxiety/panic attack. Could you help me?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:22:29.430434Z","iopub.execute_input":"2024-11-02T14:22:29.431137Z","iopub.status.idle":"2024-11-02T14:22:37.115417Z","shell.execute_reply.started":"2024-11-02T14:22:29.431095Z","shell.execute_reply":"2024-11-02T14:22:37.114319Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"user\nHello, I am in the middle of a severe anxiety/panic attack. Could you help me?\nmodel\nHi, Thanks for your query. I can understand your concern. I would suggest you to take deep breaths and relax. You can also try to focus on your breathing. You can also try to distract yourself by doing something else. You can also try to relax your muscles. You can also try to meditate. You can also try to do some exercise. You can also try to eat something healthy. You can also try to get some sleep. You can also try to take some medication. I hope this helps.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading from Pretrained","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"samarth1029/Gemma-2-2b-baymax\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \nlow_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:32:57.867642Z","iopub.execute_input":"2024-11-02T14:32:57.868059Z","iopub.status.idle":"2024-11-02T14:33:10.055803Z","shell.execute_reply.started":"2024-11-02T14:32:57.868021Z","shell.execute_reply":"2024-11-02T14:33:10.054953Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46a83f6e8e6455a80526dc91068540f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9f273d9130430b9b6f3469b5c04b0e"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"The cure to fever is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=10)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-11-02T14:33:17.423288Z","iopub.execute_input":"2024-11-02T14:33:17.423675Z","iopub.status.idle":"2024-11-02T14:33:23.234737Z","shell.execute_reply.started":"2024-11-02T14:33:17.423636Z","shell.execute_reply":"2024-11-02T14:33:23.233767Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"The cure to fever is to give the patient a cold drink. This is\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}